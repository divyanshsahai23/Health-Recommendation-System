# Install PySpark
!pip install pyspark

# Now you can import and use Spark
from pyspark.sql import SparkSession
from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, MinMaxScaler, PCA
from pyspark.ml.clustering import KMeans
from pyspark.ml.recommendation import ALS, ALSModel
from pyspark.ml.evaluation import RegressionEvaluator

# Optional: for plotting
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

# Create Spark session
spark = SparkSession.builder.appName("ColabSpark").getOrCreate()

!pip install findspark

# Python 3.9.1
import findspark
findspark.init()
from pyspark.sql import SparkSession
from pyspark.ml.feature import StringIndexer, OneHotEncoder
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.feature import MinMaxScaler
from pyspark.ml.feature import PCA
from pyspark.ml.clustering import KMeans
from pyspark.ml.recommendation import ALS,ALSModel
from pyspark.ml.evaluation import RegressionEvaluator
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

from google.colab import files
uploaded = files.upload()

from google.colab import drive
drive.mount('/content/drive')

from pyspark.sql import SparkSession

# Reduce Spark log verbosity
spark = SparkSession.builder.appName("HealthRecSystem").getOrCreate()
spark.sparkContext.setLogLevel("ERROR")  # Hide unnecessary warnings

# Load datasets
df = spark.read.csv("sym_dia_diff.csv", header=True, inferSchema=True)
sym = spark.read.csv("symptoms.csv", header=True, inferSchema=True)
dia = spark.read.csv("diagnosis.csv", header=True, inferSchema=True)

# Show data samples
print("Symptoms Data:")
sym.show(4)
print("Diagnosis Data:")
dia.show(4)
print("Difference Data:")
df.show(4)

print("feature engineering started")
string_indexer=StringIndexer(inputCol="symptom",outputCol="symptom_index")
model=string_indexer.setHandleInvalid("skip").fit(df)
indexed=model.transform(df)
indexed.show(4)
encoder=OneHotEncoder(dropLast=False,inputCols=["symptom_index"],outputCols=["symptom_vec"])
encoded=encoder.fit(indexed).transform(indexed)
df=encoded
df.show(4)

string_indexer=StringIndexer(inputCol="diagnose",outputCol="diagnose_index")
model=string_indexer.setHandleInvalid("skip").fit(df)
indexed=model.transform(df)
indexed.show(4)

string_indexer=StringIndexer(inputCol="diagnose",outputCol="diagnose_index")
model=string_indexer.setHandleInvalid("skip").fit(df)
indexed=model.transform(df)
indexed.show(4)

print("Feature engineering completed")
print("Feature scaling started")

# Check existing columns
print("Available columns before scaling:", df.columns)

# Define correct column names for scaling
cols = ["symptom_vec", "diagnose_vec"]  # Ensure these exist in df

# Apply MinMaxScaler only if the column is not already scaled
for col in cols:
    scaled_col = col + "_scaled"
    if scaled_col not in df.columns:  # Avoid duplicate scaling
        scaler = MinMaxScaler(inputCol=col, outputCol=scaled_col)
        scaler_model = scaler.fit(df)
        df = scaler_model.transform(df)

df.show(4)

# Vector Assembler for feature combination
vec_assembler = VectorAssembler(inputCols=["symptom_vec_scaled", "diagnose_vec_scaled"], outputCol="features")
df = vec_assembler.transform(df)

df.show(4)
print("Feature scaling completed")


from pyspark.ml.recommendation import ALS, ALSModel
from pyspark.sql import SparkSession

# Initialize Spark
spark = SparkSession.builder.appName("ALS_Model").getOrCreate()
spark.sparkContext.setLogLevel("ERROR")  # Suppress excessive logs

print("Model training started")

# Drop null values in the required columns
df = df.na.drop(subset=['syd', 'diagnose_index', 'wei'])

# Drop prediction column if it exists
if "prediction" in df.columns:
    df = df.drop("prediction")

df.show(4)
print("Type of df:", type(df))

# Split data into training and testing sets
splits = df.randomSplit([0.75, 0.25], seed=24)
train = splits[0]
test = splits[1]

print("Training data size:", train.count())
print("Test data size:", test.count())

# ALS Model
rec = ALS(
    maxIter=10, regParam=0.01, userCol="syd",
    itemCol="diagnose_index", ratingCol="wei",
    nonnegative=True, coldStartStrategy="drop"
)

# Train model
rec_model = rec.fit(train)

# ✅ Overwrite existing model instead of failing
rec_model.write().overwrite().save("./models/als_model")

print("Model training completed")

# ✅ Use ALSModel.load() instead of ALS.load()
print("Loading model...")
rec_saved_model = ALSModel.load("./models/als_model")  # ✅ FIXED LINE
print("Model loaded successfully")


def get_id(symptom):
    return int(df.filter(df["symptom"]==symptom).select("syd").collect()[0][0])
final_symptoms = []
# Get user input for symptoms
symptoms = list(input("Enter symptoms comma-separated: ").split(","))
print("Symptoms inputted:", symptoms)
for i in symptoms:
    final_symptoms.append(get_id(i))

# Get the recommended diagnosis
df1a = df.filter(df["syd"].isin(final_symptoms)).select('syd', 'did', 'diagnose_index', 'diagnose').orderBy('wei', ascending=False)
df1a.show(4)
recs = rec_saved_model.transform(df1a).orderBy('prediction', ascending=False)
print("Recommended diagnosis for symptoms:", symptoms)
recs.show(10, False)
spark.stop()











